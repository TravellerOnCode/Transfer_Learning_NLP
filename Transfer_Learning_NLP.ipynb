{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n# Loading Dependencies\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n#from kaggle_datasets import KaggleDatasets\nimport transformers\n\nfrom transformers import DistilBertTokenizer, RobertaTokenizer\nfrom transformers import BertTokenizer, BertConfig\nfrom transformers import BertForTokenClassification, AdamW\n\nimport os\nfrom tqdm import tqdm,trange","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_file_address = '/Kaggle/input/news_category_dataset/News_Category_Dataset_v2.json'\n# Fillna method can make same sentence with same sentence name\n#df_data = pd.read_csv(data_file_address,sep=\",\",encoding=\"latin1\").fillna(method='ffill')\ndf_data = pd.read_json('/kaggle/input/news-category-dataset/News_Category_Dataset_v2.json',lines=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#working with only these datas\n#data = df_data[df_data['category'] in ['POLITICS','ENTERTAINMENT','BUSINESS','SPORTS','TECH']]\ndata = df_data.loc[df_data['category'].isin(['POLITICS','ENTERTAINMENT','BUSINESS','SPORTS','TECH'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[['category','headline']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encode the Labels\nlabel_index = {'POLITICS':0,'ENTERTAINMENT':4,'BUSINESS':2,'SPORTS':3,'TECH':1}\n\ndata['category'] = [label_index[i] for i in data['category']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_ = data.sort_values(by='category',ascending=True)\ndata_.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_.category.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_ = data_.iloc[18000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_.category.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = shuffle(data)\ndata.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from transformers import DistilBertTokenizer, RobertaTokenizer, \ndistil_bert = 'distilbert-base-uncased' # Pick any desired pre-trained model\nroberta = 'roberta-base-uncase'\n\n# Defining DistilBERT tokonizer\ntokenizer = DistilBertTokenizer.from_pretrained(distil_bert)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(sentences, tokenizer,MAX_LENGTH=512):\n    input_ids, input_masks, input_segments = [],[],[]\n    for sentence in tqdm(sentences):\n        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=MAX_LENGTH, pad_to_max_length=True, \n                                             return_attention_mask=True, return_token_type_ids=True)\n        input_ids.append(inputs['input_ids'])\n        input_masks.append(inputs['attention_mask'])\n        input_segments.append(inputs['token_type_ids'])        \n        \n    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids, input_masks, input_segments = tokenize(data.headline.astype(str),tokenizer)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = data.category.values\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#IMP DATA FOR CONFIG\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 3\n#BATCH_SIZE = 16 * strategy.num_replicas_in_sync\nBATCH_SIZE = 16\n#MAX_LEN = 192","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, train_masks, test_masks,train_segs, test_segs, y_train, y_test = train_test_split(\n                                                            input_ids, input_masks, input_segments, labels, \n                                                            random_state=42, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_val, x_test,val_masks, test_masks,val_segs, test_segs, y_val , y_test = train_test_split(x_test, test_masks,test_segs, y_test,\n                                                            random_state=42, test_size=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_val, y_val))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer_model,max_len=512):\n\n  \n  input_ids_in = tf.keras.layers.Input(shape=(max_len,), name='input_token', dtype='int32')\n  #input_masks_in = tf.keras.layers.Input(shape=(max_len,), name='masked_token', dtype='int32') \n\n  embedding_layer = transformer_model(input_ids_in)[0]#, attention_mask=input_masks_in)[0]\n  cls_token = embedding_layer[:, 0, :]\n  #layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n  #layer = tf.keras.layers.GlobalMaxPool1D()(layer2)\n  #layer = tf.keras.layers.Dense(50, activation='relu')(layer)\n  #layer = tf.keras.layers.Dropout(0.2)(layer)\n  layer = tf.keras.layers.Dense(5, activation='softmax')(cls_token)\n  model = tf.keras.Model(inputs=[input_ids_in], outputs = layer)\n\n  #for layer in model.layers[:3]:\n  #  layer.trainable = False\n\n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-uncased')\n    )\n    model = build_model(transformer_layer, max_len=512)\nmodel.summary()\n\n#bert_model = build_model(transformer_model,MAX_LENGTH)\n#bert_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5) #, epsilon=1e-08, clipnorm=1.0)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=5,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = model.evaluate(x_test,y_test)\nprint(\"Test Loss -> \",accuracy[0])\nprint(\"Accuracy -> \",accuracy[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer_model,max_len=512):\n\n  input_ids_in = tf.keras.layers.Input(shape=(max_len,), name='input_token', dtype='int32')\n  #input_masks_in = tf.keras.layers.Input(shape=(max_len,), name='masked_token', dtype='int32') \n\n  embedding_layer = transformer_model(input_ids_in)[0]#, attention_mask=input_masks_in)[0]\n  #cls_token = embedding_layer[:, 0, :]\n  layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n  layer = tf.keras.layers.GlobalMaxPool1D()(layer)\n  layer = tf.keras.layers.Dense(50, activation='relu')(layer)\n  layer = tf.keras.layers.Dropout(0.2)(layer)\n  layer = tf.keras.layers.Dense(5, activation='softmax')(layer)\n  model = tf.keras.Model(inputs=[input_ids_in], outputs = layer)\n\n  for layer in model.layers[:3]:\n    layer.trainable = False\n\n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-uncased')\n    )\n    model2 = build_model(transformer_layer, max_len=512)\nmodel2.summary()\n\n#bert_model = build_model(transformer_model,MAX_LENGTH)\n#bert_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5) #, epsilon=1e-08, clipnorm=1.0)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel2.compile(optimizer=optimizer, loss=loss, metrics=[metric])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model2.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=5,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy2 = model2.evaluate(x_test,y_test)\nprint(\"Test Loss -> \",accuracy2[0])\nprint(\"Accuracy -> \",accuracy2[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trainable set to False","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-uncased')\n    )\n    model4 = build_model(transformer_layer, max_len=512)\nmodel4.summary()\n\n#bert_model = build_model(transformer_model,MAX_LENGTH)\n#bert_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5) #, epsilon=1e-08, clipnorm=1.0)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel4.compile(optimizer=optimizer, loss=loss, metrics=[metric])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model4.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=5,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer_model,max_len=512):\n\n  input_ids_in = tf.keras.layers.Input(shape=(max_len,), name='input_token', dtype='int32')\n  \n  embedding_layer = transformer_model(input_ids_in)[0]\n  layer = tf.keras.layers.GlobalMaxPool1D()(embedding_layer)\n  layer = tf.keras.layers.Dropout(0.2)(layer)\n  layer = tf.keras.layers.Dense(5, activation='softmax')(layer)\n  model = tf.keras.Model(inputs=[input_ids_in], outputs = layer)\n\n  #for layer in model.layers[:3]:\n  #  layer.trainable = False\n\n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-uncased')\n    )\n    model3 = build_model(transformer_layer, max_len=512)\nmodel3.summary()\n\n#bert_model = build_model(transformer_model,MAX_LENGTH)\n#bert_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5) #, epsilon=1e-08, clipnorm=1.0)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel3.compile(optimizer=optimizer, loss=loss, metrics=[metric])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model3.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=5,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy3 = model3.evaluate(x_test,y_test)\nprint(\"Test Loss -> \",accuracy3[0])\nprint(\"Accuracy -> \",accuracy3[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using a smaller batch size","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer_model,max_len=512):\n\n  input_ids_in = tf.keras.layers.Input(shape=(max_len,), name='input_token', dtype='int32')\n  \n  embedding_layer = transformer_model(input_ids_in)[0]\n  layer = tf.keras.layers.GlobalMaxPool1D()(embedding_layer)\n  layer = tf.keras.layers.Dropout(0.2)(layer)\n  layer = tf.keras.layers.Dense(5, activation='softmax')(layer)\n  model = tf.keras.Model(inputs=[input_ids_in], outputs = layer)\n\n  #for layer in model.layers[:3]:\n  #  layer.trainable = False\n\n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-uncased')\n    )\n    model5 = build_model(transformer_layer, max_len=512)\nmodel5.summary()\n\n#bert_model = build_model(transformer_model,MAX_LENGTH)\n#bert_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5) #, epsilon=1e-08, clipnorm=1.0)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel5.compile(optimizer=optimizer, loss=loss, metrics=[metric])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model5.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=5,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy45 = model3.evaluate(x_test,y_test)\nprint(\"Test Loss -> \",accuracy45[0])\nprint(\"Accuracy -> \",accuracy45[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training the First Model with Batch Size 16","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-uncased')\n    )\n    model6 = build_model(transformer_layer, max_len=512)\nmodel6.summary()\n\n#bert_model = build_model(transformer_model,MAX_LENGTH)\n#bert_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5) #, epsilon=1e-08, clipnorm=1.0)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel6.compile(optimizer=optimizer, loss=loss, metrics=[metric])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model6.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=5,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy6 = model3.evaluate(x_test,y_test)\nprint(\"Test Loss -> \",accuracy6[0])\nprint(\"Accuracy -> \",accuracy6[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# add the EOS token as PAD token to avoid warnings\nmodel = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode context the generation is conditioned on\ninput_ids = tokenizer.encode('We should stay home now and enjoy because', return_tensors='tf')\n\n# generate text until the output length (which includes the context length) reaches 50\ngreedy_output = model.generate(input_ids, max_length=50)\n\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(greedy_output[0], skip_special_tokens=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set seed to reproduce results. Feel free to change the seed though to get different results\ntf.random.set_seed(0)\n\n# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\nsample_outputs = model.generate(\n    input_ids,\n    do_sample=True, \n    max_length=50, \n    top_k=50, \n    top_p=0.95, \n    num_return_sequences=3\n)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" Greedy Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\nprint('\\n')\n\n\nprint(\"After modifying the parameters - Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"{}: {} \\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" Greedy Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\nprint('\\n')\n\n\nprint(\"After modifying the parameters - Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {} \\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}